{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6307310-bb51-489b-ba2d-50887a629212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook demonstrates a complete ETL pipeline for the UCI Heart Disease dataset.\n",
    "# We will load, clean, preprocess, and transform the data using Pandas and Scikit-learn,\n",
    "# then export the processed dataset and pipeline for future modeling tasks.\n",
    "# 2. Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c903ba-15e2-44d3-8913-1af0020d7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Load the dataset\n",
    "# URL with data, no header row in original data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ca340de-7085-47bf-8db3-bb3a5fa355ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
      "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
      "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
      "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
      "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
      "\n",
      "   slope   ca  thal  target  \n",
      "0    3.0  0.0   6.0       0  \n",
      "1    2.0  3.0   3.0       2  \n",
      "2    2.0  2.0   7.0       1  \n",
      "3    3.0  0.0   3.0       0  \n",
      "4    1.0  0.0   3.0       0  \n"
     ]
    }
   ],
   "source": [
    "# Column names from dataset documentation\n",
    "column_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\",\n",
    "                \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "\n",
    "# Read dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=column_names, na_values='?')\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70d3feed-01b6-4009-9b86-d0ba9751eeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        299 non-null    float64\n",
      " 12  thal      301 non-null    float64\n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n",
      "None\n",
      "\n",
      "Missing values by column:\n",
      " age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          4\n",
      "thal        2\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.info())\n",
    "print(\"\\nMissing values by column:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b348ea44-7870-4e9e-b4b2-f1cd556cabe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    164\n",
      "1    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Target Variable Overview\n",
    "\n",
    "Target variable indicates presence of heart disease (0 = no disease, 1-4 = disease).\n",
    "\n",
    "We will convert this into a binary classification target: 0 = no disease, 1 = disease present.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Transform target into binary outcome\n",
    "df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4216a71-0c60-4bde-b314-dc4a99f5c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Handling Missing Values & Data Types\n",
    "\n",
    "- 'ca' and 'thal' columns have missing values.\n",
    "- Some columns are categorical (cp, restecg, slope, ca, thal).\n",
    "- Prepare column lists for numeric and categorical separately.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Define numeric and categorical columns for pipeline\n",
    "numeric_features = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "categorical_features = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cb32644-47f2-4f0a-99cf-50f05e85fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Build Preprocessing Pipelines\n",
    "\n",
    "- Numeric pipeline: impute missing with median, then scale.\n",
    "- Categorical pipeline: impute missing with mode, one-hot encode.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Numeric transformer pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical transformer pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Full ColumnTransformer applying preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81842ca7-84c5-4907-8371-b1ab2085ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature shape: (303, 28)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Apply Preprocessing Pipeline to Features\n",
    "\n",
    "Separate target and features, fit and transform features using pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Processed feature shape:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1137ad20-7933-4199-8a52-380ae142ab33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heart_disease_preprocessor.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Export Processed Dataset and Preprocessing Pipeline\n",
    "\n",
    "- Save transformed dataset to CSV.\n",
    "- Save pipeline as a pickle file for reuse in modeling.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Save processed data to CSV (as dense array)\n",
    "processed_df = pd.DataFrame(X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed)\n",
    "processed_df['target'] = y.values  # Append target back\n",
    "processed_df.to_csv('heart_disease_processed.csv', index=False)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(preprocessor, 'heart_disease_preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf180-3e92-4aae-9fca-2216f90276b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
